---
title: "Machine Learning Prediction of Human Activity Recognition (HAR)"
output: html_document
---
###Synopsis
This report presents prediction outcomes of manner in which various subjects performed a series of physical exercises. The main data source contains information collected from wearable accelerometers for 5 manners (classess) of weight lifting exercises with 10 repetitions completed by 6 healthy subjects. The classess corresponds to the following manners of exercising:  
 - Exactly according to the specification (Class A)  
 - Throwing the elbows to the front (Class B)  
 - Lifting the dumbbell only halfway (Class C)   
 - Lowering the dumbbell only halfway (Class D)  
 - Throwing the hips to the front (Class E)
 
The goal of the study is to predict the manner in which the subjects performed the physical activities, in an attempt to assess the correctness of these exercises.   
The data has been generated by the following resource found at http://groupware.les.inf.puc-rio.br/har.
More information about the benchmark data and overall study can be found in the following paper:  
Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.


###Data Set
For the purpose of this study, training and test data sets have already been provided.

The training data set is available here:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data set is available here:
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

###Identified Objectives
The following step objectives have been identified that lead to the study/assignment completion:  
**1. Select and build the training model used to predict the classe variable**         
**2. Perform cross validation**    
**3. Determine the expected out of sample error**    
**4. State the conclusions and the analysis choices that led to the conclusions**  

###Basic Exploratory Data Analysis
The first step in peforming some basic exploratory analysis is to download the training and test datasets:
```{r, eval=TRUE, echo=FALSE}
#Download training data
trainToDownload <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
download.file(trainToDownload, destfile = "trainingData.csv")
datTrain = read.csv("trainingData.csv")

#Download test data
testToDownload <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
download.file(testToDownload, destfile = "testData.csv")
datTest = read.csv("testData.csv")
```

A check for missing values was performed and indicates that `r mean(is.na(datTrain))*100`% of the training dataset and  `r mean(is.na(datTest))*100`% have missing values. These are fairly large fractions so that eliminating them may  influence the outcome prediction. Another option would be to impute the missing values. The size of training and summary for our predicted feature of interest, **classe**, are outlined below.
The structure of the training data reveals that the first feature is irrelevant because it indicates each count of an instance, so it would be removed.
```{r, eval=TRUE, echo=TRUE}
#Summarize training dataset
dim(datTrain)
#str(datTrain)
datTrain <- datTrain[c(-1)]
summary(datTrain$classe)
```

Surprisingly, the size of the provided test data set is signficantly smaller:
```{r, eval=TRUE, echo=TRUE}
#Summarize training dataset
dim(datTest)
#str(datTrain)
```

###Data Splitting and Pre-Processing
It can be observed that the the test set provided is very small compared to the training data set.
The large data set should be split to extract a test data set that can be used to properly validate the training model:

```{r, eval=TRUE, echo=TRUE}
#Split Data
library(caret)
set.seed(2000)
splitTrain <- createDataPartition(y=datTrain$classe, p=0.7, list=FALSE)
trainFit <- datTrain[splitTrain, ]
testFit <- datTrain[-splitTrain, ]
dim(trainFit); dim(testFit)
```

As part of the data preprocessing, the zero and the near-zero variance predictors are identified.
The analysis indicates that there are no zero predictors, but `r library(caret); dataNZV <- nearZeroVar(trainFit, saveMetrics=TRUE); nrow(dataNZV[dataNZV[,"nzv"] > 0, ])` features out of a total of `r ncol(trainFit)-1` predictors are highly unbalanced near zero predictors that can have a significant biased influence on the cross-validation process and overall model. As such, these predictors should be and have been eliminated from the training dataset. A look at the new dimension for the training dataset indicate that they have been eliminated:

```{r, eval=TRUE, echo=TRUE}
#Eliminate the near-zero predictors
library(caret)
dataNZV <- nearZeroVar(trainFit)
#dataNZV
#dim(dataNZV[dataNZV[,"nzv"] > 0, ])
datTrain2 <- trainFit[, -dataNZV]
dim(datTrain2)
```

Additional data pre-processing also has to be applied to the training, test and validation data set so that type of variables and column names are matched before fitting the training model. 
First, the training set has to be matched with the validation set. The first four columns will also be removed since seem irrelevant to the predicted features.
```{r, eval=TRUE, echo=TRUE}
#Cleanup the columnns with NA from the validation test
ckNA <- datTest[colSums(!is.na(datTest)) > 0]

colTest <- colnames(ckNA)
training2 <- datTrain2[, which(names(datTrain2) %in% colTest)]
training3 <- cbind(training2, datTrain2[, length(datTrain2)])
colnames(training3)[length(training3)] <- "classe"

ckNA <- ckNA[, which(names(ckNA) %in% colnames(training3))]
ckNA <- ckNA[c(-1,-2,-3,-4)]
training3 <- training3[c(-1,-2,-3,-4)]
```

The test set will also be matched based on the same variable type and names with the training and validation data sets:
```{r, eval=TRUE, echo=TRUE}
testing2 <- testFit[, which(names(testFit) %in% colTest)]
testing3 <- cbind(testing2, testFit[, length(testFit)])
colnames(testing3)[length(testing3)] <- "classe"

testing3 <- testing3[c(-1,-2,-3,-4)]
```

###Model Training with Tree Method (rcart)
The rcart tree method has been chosen to train the model due to the categorical nature of the predicted value and the availability of very many predictor features. The tree methods handle this type of classification prediction better than other methods.
```{r, eval=TRUE, echo=TRUE}
##Use 10-fold cross-validation
setCV <- trainControl(method = "repeatedcv",
                      number = 10,
                         repeats = 3)

set.seed(1500)
rpartFit <- train(classe ~ ., data = training3, 
                 method = "rpart", trControl = setCV)
                 
rpartFit

#plot the classification tree
library(rattle)
fancyRpartPlot(rpartFit$finalModel)
```

Predict the values and assess the accuracy:
```{r, eval=TRUE, echo=TRUE}
predictRPART <- predict(rpartFit, newdata = testing3)
confusionMatrix(predictRPART, testing3$classe)$overall[1]
```

It can be observed that the overall accuracy of the tree method is low. Some options to include the accuracy may be to iteratively split and prune variables or resplit the training dataset. since little information is known about the variables, resplitting the dataset remains the only feasible option. However, it is not expected that resplitting will yield signifcantly better results. As such, a better training model was chosen to try to increase the prediction accuracy.


###Model Training with Random Forest
The random forest method is known to provide high accuracy, but it is very computationally expensive using the train method with rf option. HOwever, using the randomforest function will provide much faster results:
```{r, eval=TRUE, echo=TRUE}
set.seed(3456)

library(randomForest)
rfFit <- randomForest(classe ~ ., data=training3, 
                     verbose=TRUE)
#Predict the values and assess the accuracy:
predictRF <- predict(rfFit, newdata = testing3)
confusionMatrix(predictRF, testing3$classe)$overall[1]

predFinal <- predict(rfFit, newdata = ckNA)
```

As observed, the accuracy of the random forest model is much higher and in fact is 99.8% versus 50% as compared to the tree model.

###Summary and Conclusions
The study was performed with two training models.Data pre-processing included data splitting, elimination of zero and near-zero variance predictors, data cleaning with feature selection and removal.
The following conclusions can be drawn:
 - Random Forest model is much more accurate than a tree-based model (RPART)
 - Random forest is computationally expensive with the train function but much faster with randomforest function
 - The pre-processing, cross-validation and feature selection are probably important factors in achieving high accuracy, but for these data sets the type of training model was considerably more effective. 
